// Module included in the following assemblies:
//
// assembly_managing-nvidia-vgpu-devices

:_content-type: PROCEDURE
[id="proc_setting-up-nvidia-vgpu-devices_{context}"]
= Setting up NVIDIA vGPU devices on the host

[NOTE]
====
Before installing the NVIDIA vGPU driver on the guest operating system, you need to understand the licensing requirements and obtain the correct license credentials.
====

[id='prerequisites-{context}']
.Prerequisites

* Your GPU device supports virtual GPU (vGPU) functionality.

* Your system is listed as a validated server hardware platform.

For more information about supported GPUs and validated platforms, see link:https://www.nvidia.com/en-us/data-center/resources/vgpu-certified-servers/[NVIDIA vGPU CERTIFIED SERVERS] on www.nvidia.com.

//== Before You Begin
.Procedure

. Download and install the NVIDIA driver for the host. For information on getting the driver, see link:https://www.nvidia.com/Download/index.aspx?lang=en-us[the Drivers page on the NVIDIA website].
[discrete]
//== Configuring the Host

. If the NVIDIA software installer did not create the [filename]`/etc/modprobe.d/nvidia-installer-disable-nouveau.conf` file, create it manually.

. Open [filename]`/etc/modprobe.d/nvidia-installer-disable-nouveau.conf` file in a text editor and add the following lines to the end of the file:
+
[source,bash,subs=+quotes]
----
blacklist nouveau
options nouveau modeset=0
----

. Regenerate the initial ramdisk for the current kernel, then reboot:
+
[source,bash,subs=+quotes]
----
# *dracut --force*
# *reboot*
----
+
Alternatively, if you need to use a prior supported kernel version with mediated devices, regenerate the initial ramdisk for all installed kernel versions:
+
[source,bash,subs=+quotes]
----
# *dracut --regenerate-all --force*
# *reboot*
----

. Check that the kernel loaded the [filename]`nvidia_vgpu_vfio` module:
+
[source,bash,subs=+quotes]
----
# lsmod | grep nvidia_vgpu_vfio
----

. Check that the `nvidia-vgpu-mgr.service` service is running:
+
[source,bash,subs=+quotes]
----
# systemctl status nvidia-vgpu-mgr.service
----
+
For example:
+
[source,bash,subs=+quotes]
----
# lsmod | grep nvidia_vgpu_vfio
nvidia_vgpu_vfio 45011 0
nvidia 14333621 10 nvidia_vgpu_vfio
mdev 20414 2 vfio_mdev,nvidia_vgpu_vfio
vfio 32695 3 vfio_mdev,nvidia_vgpu_vfio,vfio_iommu_type1
# systemctl status nvidia-vgpu-mgr.service
nvidia-vgpu-mgr.service - NVIDIA vGPU Manager Daemon
   Loaded: loaded (/usr/lib/systemd/system/nvidia-vgpu-mgr.service; enabled; vendor preset: disabled)
   Active: active (running) since Fri 2018-03-16 10:17:36 CET; 5h 8min ago
 Main PID: 1553 (nvidia-vgpu-mgr)
 [...]
----
// From here to the end of the procedure is unique to RHV and needs to be conditionalized as such. The entire NVIDIA topic set is unique to downstream/RHV.

. In the Administration Portal, click menu:Compute[Virtual Machines].

. Click the name of the virtual machine to go to the details view.

. Click the *Host Devices* tab.

. Click btn:[Manage vGPU]. The *Manage vGPU* dialog box opens.

. Select a vGPU type and the number of instances that you would like to use with this virtual machine.

. Select *On* for *Secondary display adapter for VNC* to add a second emulated QXL or VGA graphics adapter as the primary graphics adapter for the console in addition to the vGPU.
+
[NOTE]
====
On cluster levels 4.5 and later, when a vGPU is used and the *Secondary display adapter for VNC* is set to *On*, an additional framebuffer display device is automatically added to the virtual machine. This allows the virtual machine console to be displayed before the vGPU is initialized, instead of a blank screen.
====
+
. Click btn:[Save].
