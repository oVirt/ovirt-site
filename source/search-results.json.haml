---
index: false
---

:ruby

  # Function to find the date for a file using git
  def lookup_git_date(source_file)
    git_date = `git log -1 --format="%ad" -- "#{source_file}"`
    return git_date.to_time
  end

  pages ||= []
  words ||= {}

  # Stopwords (from PHP)
  stopwords = ["a", "about", "above", "above", "across", "after",
               "afterwards", "again", "against", "all", "almost",
               "alone", "along", "already",
               "also","although","always","am","among", "amongst",
               "amoungst", "amount",  "an", "and", "another",
               "any","anyhow","anyone","anything","anyway", "anywhere",
               "are", "around", "as",  "at", "back","be","became",
               "because","become","becomes", "becoming", "been",
               "before", "beforehand", "behind", "being", "below",
               "beside", "besides", "between", "beyond", "bill", "both",
               "bottom","but", "by", "call", "can", "cannot", "cant",
               "co", "con", "could", "couldnt", "cry", "de", "describe",
               "detail", "do", "done", "down", "due", "during", "each",
               "eg", "eight", "either", "eleven","else", "elsewhere",
               "empty", "enough", "etc", "even", "ever", "every",
               "everyone", "everything", "everywhere", "except", "few",
               "fifteen", "fify", "fill", "find", "fire", "first",
               "five", "for", "former", "formerly", "forty", "found",
               "four", "from", "front", "full", "further", "get",
               "give", "go", "had", "has", "hasnt", "have", "he",
               "hence", "her", "here", "hereafter", "hereby", "herein",
               "hereupon", "hers", "herself", "him", "himself", "his",
               "how", "however", "hundred", "ie", "if", "in", "inc",
               "indeed", "interest", "into", "is", "it", "its",
               "itself", "keep", "last", "latter", "latterly", "least",
               "less", "ltd", "made", "many", "may", "me", "meanwhile",
               "might", "mill", "mine", "more", "moreover", "most",
               "mostly", "move", "much", "must", "my", "myself", "name",
               "namely", "neither", "never", "nevertheless", "next",
               "nine", "no", "nobody", "none", "noone", "nor", "not",
               "nothing", "now", "nowhere", "of", "off", "often", "on",
               "once", "one", "only", "onto", "or", "other", "others",
               "otherwise", "our", "ours", "ourselves", "out", "over",
               "own","part", "per", "perhaps", "please", "put",
               "rather", "re", "same", "see", "seem", "seemed",
               "seeming", "seems", "serious", "several", "she",
               "should", "show", "side", "since", "sincere", "six",
               "sixty", "so", "some", "somehow", "someone", "something",
               "sometime", "sometimes", "somewhere", "still", "such",
               "system", "take", "ten", "than", "that", "the", "their",
               "them", "themselves", "then", "thence", "there",
               "thereafter", "thereby", "therefore", "therein",
               "thereupon", "these", "they", "thickv", "thin", "third",
               "this", "those", "though", "three", "through",
               "throughout", "thru", "thus", "to", "together", "too",
               "top", "toward", "towards", "twelve", "twenty", "two",
               "un", "under", "until", "up", "upon", "us", "very",
               "via", "was", "we", "well", "were", "what", "whatever",
               "when", "whence", "whenever", "where", "whereafter",
               "whereas", "whereby", "wherein", "whereupon", "wherever",
               "whether", "which", "while", "whither", "who", "whoever",
               "whole", "whom", "whose", "why", "will", "with",
               "within", "without", "would", "yet", "you", "your",
               "yours", "yourself", "yourselves", "the",
               ".", "-", ":", "/", "\\"]

  # Cache results in a global session variable (restart session for updates)
  unless defined?($searchdata)

    # Make sure a line is skipped in STDOUT
    # as we'll output a progress bar (later; look for more `puts`)
    puts ''

    # We're going to scan all HTML pages, so select them all
    $htmldocs ||= sitemap.resources.select {|p| p.path.end_with?('.html')}

    # Total number of pages to index
    total = current = $htmldocs.length

    $htmldocs.each do |page|
      current -= 1

      # Output progress bar to STDOUT
      screen_width = ENV['COLUMNS'] || 72
      progress_string = "Compiling search index"
      console_width = screen_width - progress_string.length - 5
      ratio_done = (total - current).to_f / total.to_f
      bar = "#" * (ratio_done * console_width).to_i
      empty = "-" * ((1 - ratio_done) * console_width).to_i
      print "\r#{progress_string}: #{(ratio_done * 100).to_i}% #{bar}#{empty}"

      # Skip various special pages
      next if page.url.match /^\/search\//
      next if page.url.match /^\/admin\//
      next if page.url.match /^\/404/
      next if page.url.match /^\/$/
      next if page.url.match /^\/blog\/$/
      next if page.url.match /^\/blog\/tag/
      next if page.url.match /^\/blog\/author/
      next if page.url.match /\=/

      next if page.data['index'] == false

      # Discover the page title, either specified by title
      # or implied by its first header (<h1>)
      page_title = discover_title(page)

      # Skip pages that have no titles (or headers)
      next if page_title == nil

      # Use the page's metadata to discover its date
      # or fall back to discovering it via git
      page_date = defined?(page.date) ? page.date : lookup_git_date(page.source_file)

      # Assign a unique page ID based on the # of pages processed
      # (used internally for search to link pages to words and vice versa)
      page_id = pages.length

      # Use page content as-is or fall back to rendering content only
      page_content = defined?(page.body) ? page.body.to_s : page.render({layout: false}).to_s.sub(page_title, '')

      # Strip HTML (not the best way of doing this, but it's quick)
      page_content = page_content.to_s
        .gsub(/\<[^\>]*\>/, '')
        .gsub(/[\s[:space:]\p{Space}\u00A0]+/, ' ')

      # Page metadata for the search results
      page_info = {
        title: page_title,
        url: page.url,
        date: page_date.to_s,
        datestring: pretty_date(page_date.to_s),
        summary: page_content.split(/[\t\s\n[:space:]]+/).first(50).join(' ').strip + "…"
      }

      pages.push page_info

      # Regular expression used to split content into words
      wordsplit = /[\t\s\p{Space}[:space:]\p{punct}'"`\$]+/

      # Make URL fragments from simpliy splitting dirs
      # ...and also simpler keywords by splitting at other ASCII chars
      url_frags = page.url.split('/') + page.url.tr('_/-:', ' ').split(' ')

      # MediaWiki name/URL titles
      # (empty if wiki_title doesn't exist — only used for MW conversions)
      mw_frags = if page.data['wiki_title']
                   page.data['wiki_title']
                     .downcase
                     .tr('_/-:', ' ')
                     .squeeze(' ')
                     .split(' ')
                 else
                   []
                 end

      nickname = page.data.author
      author = []

      # Find author name (if applicable)
      # this data is used to make searches for authors possible
      if nickname
        nickname.downcase.split(' ').each {|n| author.push n}
        author_name(nickname).downcase.split(' ').each {|n| author.push n}
      end

      # Words found in the title (they may otherwise not show up in-page)
      titlewords = page_title.downcase.split(wordsplit).map(&:strip)

      # Extract semantic versions (including bare numbers) from the page
      sem_ver = page_content.scan(/(?:\.?\d+)+/)

      # Add found words (and versions) in the body and title together
      text = page_content.downcase.split(wordsplit) + titlewords + sem_ver

      # Clean up list of words and add URL fragments and author
      text = text.map(&:strip).sort.uniq
        .reject {|w| w.match /[=]/} + url_frags + mw_frags - stopwords + author

      # Add CJK words to the word list
      text += page_content.scan(/\p{Han}/).uniq

      # Return a unique list of words,
      # shorter than 2 and longer than 40 characters
      text.uniq.each do |word|
        next if word.empty? || word.length < 2 || word.length > 40
        words[word] ||= []
        words[word].push page_id
      end
    end

    puts ''

  end

  # Add searchdata (and cache it per Middleman session)
  $searchdata ||= {pages: pages, words: words}

-# Return searchdata (words and pages with word indexes) as JSON
= $searchdata.to_json
